{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open All Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MukundVarmaT/SHAASTRA_FUND_DL/blob/master/NLP/NLP.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Natural Language Processing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing or NLP is a branch of Machine Learning focused towards making computers understand the human language in the form of text, speech, etc. \n",
    "\n",
    "## Why is NLP difficult?\n",
    "There are various challenges in NLP. Some of them are:\n",
    "- **Text Preprocessing**: Large amount of textual data when scraped from websites or extracted from documents are not clean and contain a lot of gibberish. Even something as naive as identifying word or sentence boundaries is an important task. \n",
    "- **Building context**: A word gets its meaning from the context of the sentence or the words surrounding it. Many a times, the same word can have different meanings when used at different locations. Even different words can mean the same thing (synonyms). Eg:\n",
    "    - In different sentences -  \"We went to the **bank** to rob\", \"We went fishing at the river **bank**\".\n",
    "    - In the same sentence - \"I **ran** to the store because I **ran** out of milk\". \n",
    "    - Synonyms - \"I have a **tiny** bag\", \"I have a **small** bag\"\n",
    "- **Complexity**: Human language is very complex and diverse. There are different languages, different ways in which people talk, write, spell, etc making it more and more challenging. Building a large text corpus with all these variabilities is difficult and infeasible.\n",
    "- **Word representation**: Representing words in a form understandable by machines is a very difficult task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Every large piece of text can be broken down into sentences, words, characters, etc. This process of breaking down in called as tokenization. There are different forms of tokenization - word tokenization, sentence tokenization, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow', 'this', 'is', 'great', '!', 'Natural', 'Language', 'Processing', 'is', 'indeed', 'fun', '.']\n",
      "['Wow this is great!', 'Natural Language Processing is indeed fun.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/sneezygiraffe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "text = \"Wow this is great! Natural Language Processing is indeed fun.\"\n",
    "\n",
    "# this is quite a naive task and can be done using the split method. \n",
    "# However, to make thing simpler, we will be using nltk library\n",
    "import nltk\n",
    "# tokenization is done by splitting on punctuations, etc. Hence we need to download \"punkt\"\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# word tokenization\n",
    "print(word_tokenize(text))\n",
    "# sentence tokenization\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While sentences are a strong way of representing text, they can be further broken down into words. In NLP, we mostly deal with words and how they can be represented.\n",
    "\n",
    "Lets take three sentences.\n",
    "Sentence 1: \"This movie is very scary and long\"\n",
    "Sentence 2: \"This movie is not scary and is slow\"\n",
    "Sentence 3: \"This movie is spooky and good\"\n",
    "\n",
    "Hence the vocabulary in this case would be: ‘This’, ‘movie’, ‘is’, ‘very’, ‘scary’, ‘and’, ‘long’, ‘not’, ‘slow’, ‘spooky’, ‘good’.\n",
    "\n",
    "## Bag of Words\n",
    "The idea behind BOW is that every word is replaced with its count in the particular sentence. \n",
    "\n",
    "|Sentence|This|movie|is|very|scary|and|long|not|slow|spooky|good|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|1|1|1|1|1|1|1|1|0|0|0|0|\n",
    "|2|1|1|2|0|0|1|1|0|1|0|0|\n",
    "|3|1|1|1|0|0|0|1|0|0|1|1|\n",
    "\n",
    "**Disadvantages**\n",
    "- More words implies a larger vocabulary, making the vector large and sparse. \n",
    "- In sentence 2, it can be noticed that \"is\" is given a weight 2 but does not convey much to the context of the sentence.\n",
    "\n",
    "## TF-IDF\n",
    "Lets break it down\n",
    "\n",
    "**Term Frequency (TF)**: measure of how frequenty a word t appears in document d.\n",
    "![](assets/TF.jpg)\n",
    "**Inverse Document Frequency (IDF)**: measure of how important a term is. \n",
    "![](assets/IDF.jpg)\n",
    "\n",
    "|Sentence|This|movie|is|very|scary|and|long|not|slow|spooky|good|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|1|0|0|0|0.068|0.025|0|0.068|0|0|0|0|\n",
    "|2|0|0|0|0|0.022|0|0|0.06|0.06|0|0|\n",
    "|3|0|0|0|0|0|0|0|0|0|0.08|0.08|\n",
    "\n",
    "**To conclude the TF-IDF values are higher if, the word is rare in all documents combined but frequent in a single document.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatisation\n",
    "\n",
    "Words like \"children\" & \"child\", \"cars\" & \"car\" convey the same meaning. Hence retaining them individually only makes the vectors more sparse and large. Hence its very important to remove them. \n",
    "\n",
    "Stemming is the method of using predefined rules to transform the word to its stem. Please note that the stem need not be a valid word. \n",
    "Lemmatization on the other hand will always be a valid word because the lemma will belong to a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/sneezygiraffe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Compare stemmer and lemmatizer\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize_words(words, pos='v'):\n",
    "    normalized_words = pd.DataFrame(index=words, columns=['Porter', 'Lancaster', 'Lemmatizer'])\n",
    "    for word in words:\n",
    "        normalized_words.loc[word,'Porter'] = porter.stem(word)\n",
    "        normalized_words.loc[word,'Lancaster'] = lancaster.stem(word)\n",
    "        normalized_words.loc[word,'Lemmatizer'] = lemmatizer.lemmatize(word, pos=pos)\n",
    "    return normalized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize_words(['apples', 'pears', 'tasks', 'children', 'earrings', 'dictionary', 'marriage', 'connections', 'universe', 'university'], pos='n')\n",
    "# normalise_text(['pie', 'globe', 'house', 'knee', 'angle', 'acetone', 'time', 'brownie', 'climate', 'independence'], pos='n')\n",
    "# normalise_text(['wrote', 'thinking', 'remembered', 'relies', 'ate', 'gone', 'won', 'ran', 'swimming', 'mistreated'], pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams\n",
    "\n",
    "In all the above cases, each word is considered to be independent. But to build context, its always important to look at the surrounding words.  Hence instead of splitting words individually, they are divided into chunks of words.\n",
    "\n",
    "Sentence: “The cow jumps over the moon”\n",
    "\n",
    "- Bigram: [\"the cow\", \"cow jumps\", \"jumps over\", \"over the\", \"the moon\"]\n",
    "- Trigram: [\"the cow jumps\", \"cow jumps over\", \"jumps over the\", \"over the moon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'cow'), ('cow', 'jumps'), ('jumps', 'over'), ('over', 'the'), ('the', 'moon')]\n",
      "[('The', 'cow', 'jumps'), ('cow', 'jumps', 'over'), ('jumps', 'over', 'the'), ('over', 'the', 'moon')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence = \"The cow jumps over the moon\"\n",
    "print(list(ngrams(sentence.split(), 2)))\n",
    "print(list(ngrams(sentence.split(), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  split\n",
       "0  The Rock is destined to be the 21st Century 's...      1      1\n",
       "1  The gorgeously elaborate continuation of `` Th...      1      1\n",
       "2  If you sometimes like to go to the movies to h...      1      2\n",
       "3  Emerges as something rare , an issue movie tha...      1      2\n",
       "4  Offers that rare combination of entertainment ...      1      2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# sst1 dataset from https://nlp.stanford.edu/sentiment/\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/MukundVarmaT/SHAASTRA_FUND_DL/master/NLP/assets/sst_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sneezygiraffe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>sentence_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>rock destined 21st century new conan going mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gorgeously elaborate continuation lord ring tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>sometimes like go movie fun wasabi good place ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>emerges something rare issue movie honest keen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>offer rare combination entertainment education</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  split  \\\n",
       "0  The Rock is destined to be the 21st Century 's...      1      1   \n",
       "1  The gorgeously elaborate continuation of `` Th...      1      1   \n",
       "2  If you sometimes like to go to the movies to h...      1      2   \n",
       "3  Emerges as something rare , an issue movie tha...      1      2   \n",
       "4  Offers that rare combination of entertainment ...      1      2   \n",
       "\n",
       "                                      sentence_clean  \n",
       "0  rock destined 21st century new conan going mak...  \n",
       "1  gorgeously elaborate continuation lord ring tr...  \n",
       "2  sometimes like go movie fun wasabi good place ...  \n",
       "3  emerges something rare issue movie honest keen...  \n",
       "4     offer rare combination entertainment education  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# first we need to do some cleaning\n",
    "\n",
    "# take the stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "# print(stopwords)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def process_text(text):\n",
    "    # make lower case and remove extra spaces\n",
    "    text = str(text).lower()\n",
    "    # remove punctuations\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # convert to word tokens\n",
    "    tokens = text.split()\n",
    "    # remove stop words\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    # lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    # convert back to string\n",
    "    text = \" \".join(tokens)\n",
    "    return text\n",
    "\n",
    "data[\"sentence_clean\"] = data[\"sentence\"].apply(lambda x: process_text(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "data_train, data_test = data[data[\"split\"] == 1], data[data[\"split\"] == 3]\n",
    "# get the labels\n",
    "y_train = data_train[\"label\"].values\n",
    "y_test = data_test[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "# tfidf\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "vectorizer.fit(data_train[\"sentence_clean\"])\n",
    "x_train = vectorizer.transform(data_train[\"sentence_clean\"])\n",
    "x_test = vectorizer.transform(data_test[\"sentence_clean\"])\n",
    "\n",
    "# lets visualize the tfidf vectors\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sns.heatmap(x_train.todense()[:,np.random.randint(0, x_train.shape[1], 100)]==0, vmin=0, vmax=1, cbar=False).set_title('TFIDF VIS')\n",
    "plt.show()\n",
    "\n",
    "# see how sparse it is. We can do some minimal feature selection and make it better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7890909090909091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(x_train, y_train)\n",
    "pred_y = classifier.predict(x_test)\n",
    "acc = sklearn.metrics.accuracy_score(y_test, pred_y)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "As seen before, TF-IDF, BOW, etc do not capture the context of the words, they are sparse and extremely large making it difficult to use. Word2Vec is a class of methods used to derive better word representations. \n",
    "\n",
    "Two commonly used Word2Vec techniques are:\n",
    "- CBOW\n",
    "- SkipGram\n",
    "\n",
    "![](assets/word2vec.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q --show-progress 'http://nlp.stanford.edu/data/glove.6B.zip' -O 'glove.zip'\n",
    "!unzip -j -q 'glove.zip' 'glove.6B.300d.txt' -d './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>sentence_clean</th>\n",
       "      <th>sentence_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>rock destined 21st century new conan going mak...</td>\n",
       "      <td>[rock, destined, 21st, century, new, conan, go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gorgeously elaborate continuation lord ring tr...</td>\n",
       "      <td>[gorgeously, elaborate, continuation, lord, ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>sometimes like go movie fun wasabi good place ...</td>\n",
       "      <td>[sometimes, like, go, movies, fun, wasabi, goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>emerges something rare issue movie honest keen...</td>\n",
       "      <td>[emerges, something, rare, issue, movie, hones...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>offer rare combination entertainment education</td>\n",
       "      <td>[offers, rare, combination, entertainment, edu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  split  \\\n",
       "0  The Rock is destined to be the 21st Century 's...      1      1   \n",
       "1  The gorgeously elaborate continuation of `` Th...      1      1   \n",
       "2  If you sometimes like to go to the movies to h...      1      2   \n",
       "3  Emerges as something rare , an issue movie tha...      1      2   \n",
       "4  Offers that rare combination of entertainment ...      1      2   \n",
       "\n",
       "                                      sentence_clean  \\\n",
       "0  rock destined 21st century new conan going mak...   \n",
       "1  gorgeously elaborate continuation lord ring tr...   \n",
       "2  sometimes like go movie fun wasabi good place ...   \n",
       "3  emerges something rare issue movie honest keen...   \n",
       "4     offer rare combination entertainment education   \n",
       "\n",
       "                                     sentence_tokens  \n",
       "0  [rock, destined, 21st, century, new, conan, go...  \n",
       "1  [gorgeously, elaborate, continuation, lord, ri...  \n",
       "2  [sometimes, like, go, movies, fun, wasabi, goo...  \n",
       "3  [emerges, something, rare, issue, movie, hones...  \n",
       "4  [offers, rare, combination, entertainment, edu...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean again but without lemmatization\n",
    "def process_text(text):\n",
    "    # make lower case and remove extra spaces\n",
    "    text = str(text).lower()\n",
    "    # remove punctuations\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # convert to word tokens\n",
    "    tokens = text.split()\n",
    "    # remove stop words\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "data[\"sentence_tokens\"] = data[\"sentence\"].apply(lambda x: process_text(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 8296\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_freq = Counter()\n",
    "for i in data[\"sentence_tokens\"]:\n",
    "    word_freq.update(i)\n",
    "words = [w for w in word_freq.keys() if word_freq[w] > 1]\n",
    "word_map = {k: v + 1 for v, k in enumerate(words)}  \n",
    "word_map['<unk>'] = len(word_map) + 1\n",
    "word_map['<pad>'] = 0\n",
    "print(\"vocabulary size: {}\".format(len(word_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding...\n",
      "Number of words read:  8015\n",
      "Number of OOV:  281\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "vocab = set(word_map.keys())\n",
    "print(\"Loading embedding...\")\n",
    "cnt = 0\n",
    "with open(\"glove.6B.300d.txt\", 'r', encoding='utf-8') as f:\n",
    "    emb_dim = len(f.readline().split(' ')) - 1 \n",
    "\n",
    "embeddings = torch.FloatTensor(len(vocab), emb_dim)\n",
    "bias = np.sqrt(3.0 / embeddings.size(1))\n",
    "torch.nn.init.uniform_(embeddings, -bias, bias)\n",
    "\n",
    "for line in open(\"glove.6B.300d.txt\", 'r', encoding='utf-8'):\n",
    "    line = line.split(' ')\n",
    "    emb_word = line[0]\n",
    "    embedding = list(map(lambda t: float(t), filter(lambda n: n and not n.isspace(), line[1:])))\n",
    "    if emb_word not in vocab:\n",
    "        continue\n",
    "    else:\n",
    "        cnt+=1\n",
    "    embeddings[word_map[emb_word]] = torch.FloatTensor(embedding)\n",
    "\n",
    "print(\"Number of words read: \", cnt)\n",
    "print(\"Number of OOV: \", len(vocab)-cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "\n",
    "writer = SummaryWriter(\"emb_vis\")\n",
    "writer.add_embedding(\n",
    "    embeddings,\n",
    "    metadata=vocab\n",
    ")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a8c7a3434fd5b8cb\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a8c7a3434fd5b8cb\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"emb_vis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6896)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "interp_embed = embeddings[word_map[\"king\"]] - embeddings[word_map[\"man\"]] + embeddings[word_map[\"woman\"]]\n",
    "cos(interp_embed, embeddings[word_map[\"queen\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Neural Networks\n",
    "\n",
    "**RNN**\n",
    "\n",
    "Traditional Neural Networks inherently do not remember/recall the previous words before predicting the current word. Hence they are not apt for language understanding as there is an order in the words which provides the context. Recurrent Neural Networks or RNN's address this issue and help retain past information for the current prediction.\n",
    "\n",
    "![](assets/rnn.png)\n",
    "\n",
    "**LSTM**\n",
    "\n",
    "However, RNN's tend to fail when given a large sequence. Hence came LSTM's which can model long-range dependencies. Let's try to understand how?\n",
    "\n",
    "![](assets/lstm-1.png)\n",
    "\n",
    "**Cell state**\n",
    "\n",
    "Something like a conveyor belt carrying information through the entire chain, with minor alterations at every step. These alterations are done using gates (3 to be exact). They help propagate information based on requirement. Every gating layer consists of a sigmoid operation which determines how much information passes through (1-let everything and 0-let nothing through).\n",
    "\n",
    "![](assets/lstm-2.png)\n",
    "\n",
    "**Gates**\n",
    "\n",
    "- Modify existing information\n",
    "![](assets/lstm-3.png)\n",
    "- Add new information from the current state\n",
    "![](assets/lstm-4.png)\n",
    "- Choose what the output is going to be\n",
    "![](assets/lstm-5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class LSTMCls(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, embedding, hid_dim, n_cls):\n",
    "        super(LSTMCls, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim).from_pretrained(embedding, freeze=False)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim)\n",
    "        self.fc = nn.Linear(hid_dim, n_cls)\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).transpose(0,1)\n",
    "        h_0 = Variable(torch.zeros(1, x.shape[1], self.hid_dim).to(x.device))\n",
    "        c_0 = Variable(torch.zeros(1, x.shape[1], self.hid_dim).to(x.device))\n",
    "        _, (x, _) = self.lstm(x, (h_0, c_0))\n",
    "        x = self.fc(x[-1])\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "class CNNCls(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, embedding, fmaps, strides, dropout_factor, num_classes):\n",
    "        super(CNNCls, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim).from_pretrained(embedding, freeze=False)\n",
    "        conv_layers = [nn.Conv2d(1, fmaps, (stride, emb_dim), padding=(stride-1, 0)) for stride in strides]\n",
    "        self.conv_layers = nn.Sequential(*conv_layers)\n",
    "        self.dropout = nn.Dropout(dropout_factor)\n",
    "        self.fc = nn.Linear(len(conv_layers)*fmaps, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.conv_layers]\n",
    "        x = [F.max_pool1d(c, c.size(2)).squeeze(2) for c in x]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "class ClsDataset(Dataset):\n",
    "    def __init__(self, word_map, data, split=\"train\"):\n",
    "        if split == \"train\":\n",
    "            data = data[data[\"split\"] == 1]\n",
    "        else:\n",
    "            data = data[data[\"split\"] == 3]\n",
    "        data = data.reset_index(drop=True)\n",
    "        self.sentence = data[\"sentence_tokens\"]\n",
    "        self.label = data[\"label\"]\n",
    "        self.word_map = word_map\n",
    "    \n",
    "    def __getitem__(self, indx):\n",
    "        sent = self.sentence[indx]\n",
    "        label = self.label[indx]\n",
    "        sent = [self.word_map.get(w, self.word_map[\"<unk>\"]) for w in sent]\n",
    "        return torch.LongTensor(sent), label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentence)\n",
    "\n",
    "def pad_collate(batch):\n",
    "    sent, label = zip(*batch)\n",
    "    sent_pad = pad_sequence(sent, batch_first=True, padding_value=0)\n",
    "    return sent_pad, torch.LongTensor(label)    \n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "cls_model = LSTMCls(len(word_map), embeddings.shape[1], embeddings, 256, 2).to(device)\n",
    "# cls_model = CNNCls(len(word_map), embeddings.shape[1], embeddings, 100, [3,4,5], 0.5, 2).to(device)\n",
    "train_dset = ClsDataset(word_map, data)\n",
    "test_dset = ClsDataset(word_map, data, split=\"test\")\n",
    "\n",
    "train_loader = DataLoader(train_dset, batch_size=32, num_workers=4, shuffle=True, collate_fn=pad_collate)\n",
    "test_loader = DataLoader(test_dset, batch_size=32, num_workers=4, shuffle=False, collate_fn=pad_collate)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Epoch: 0\n",
      "Progress: [####################] 100% train loss: 0.5349, acc: 0.722222\n",
      "Progress: [####################] 100% test acc: 0.8074                \n",
      "---------------\n",
      "Epoch: 1\n",
      "Progress: [####################] 100% train loss: 0.3428, acc: 0.858888\n",
      "Progress: [####################] 100% test acc: 0.7976                \n",
      "---------------\n",
      "Epoch: 2\n",
      "Progress: [####################] 100% train loss: 0.2246, acc: 0.916666\n",
      "Progress: [####################] 100% test acc: 0.8048                \n",
      "---------------\n",
      "Epoch: 3\n",
      "Progress: [####################] 100% train loss: 0.1474, acc: 0.948686\n",
      "Progress: [####################] 100% test acc: 0.8072                \n",
      "---------------\n",
      "Epoch: 4\n",
      "Progress: [####################] 100% train loss: 0.0909, acc: 0.968888\n",
      "Progress: [####################] 100% test acc: 0.7859                \n",
      "---------------\n",
      "Epoch: 5\n",
      "Progress: [####################] 100% train loss: 0.0718, acc: 0.978383\n",
      "Progress: [####################] 100% test acc: 0.8024                \n",
      "---------------\n",
      "Epoch: 6\n",
      "Progress: [####################] 100% train loss: 0.0605, acc: 0.981313\n",
      "Progress: [####################] 100% test acc: 0.7838                \n",
      "---------------\n",
      "Epoch: 7\n",
      "Progress: [####################] 100% train loss: 0.0832, acc: 0.974848\n",
      "Progress: [####################] 100% test acc: 0.7811                \n",
      "---------------\n",
      "Epoch: 8\n",
      "Progress: [####################] 100% train loss: 0.0474, acc: 0.985353\n",
      "Progress: [####################] 100% test acc: 0.7943                \n",
      "---------------\n",
      "Epoch: 9\n",
      "Progress: [####################] 100% train loss: 0.04, acc: 0.9865 5 4\n",
      "Progress: [####################] 100% test acc: 0.7916                \n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "optim = torch.optim.Adam(cls_model.parameters(), lr=1e-3)\n",
    "\n",
    "# non tqdm progress bar UwU\n",
    "def progress_bar(progress = 0, status = \"\", bar_len = 20):\n",
    "    status = status.ljust(30)\n",
    "    block = int(round(bar_len*progress))\n",
    "    text = \"\\rProgress: [{}] {}% {}\".format( \"#\"*block + \"-\"*(bar_len-block), round(progress*100,2), status)\n",
    "    print(text, end=\"\")\n",
    "    if progress == 1:\n",
    "        print()\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(\"---------------\")\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    cls_model.train()\n",
    "    loss_cntr, acc_cntr = [], []\n",
    "    for indx, (sent, cls) in enumerate(train_loader):\n",
    "        sent, cls = sent.to(device), cls.to(device)\n",
    "        out = cls_model(sent)\n",
    "        loss = F.nll_loss(out, cls)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # log metrics\n",
    "        loss_cntr.append(loss.item())\n",
    "        pred_choice = out.max(1)[1]\n",
    "        correct = pred_choice.eq(cls.long().data).cpu().sum()\n",
    "        acc_cntr.append(correct.item()/sent.shape[0])\n",
    "\n",
    "        progress_bar(indx/len(train_loader), status=f\"train loss: {round(np.mean(loss_cntr), 4)}, acc: {round(np.mean(acc_cntr), 4)}\")\n",
    "    progress_bar(1, status=f\"train loss: {round(np.mean(loss_cntr), 4)}, acc: {round(np.mean(acc_cntr), 4)}\")\n",
    "    \n",
    "    cls_model.eval()\n",
    "    acc_cntr = []\n",
    "    for indx, (sent, cls) in enumerate(test_loader):\n",
    "        sent, cls = sent.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = cls_model(sent)\n",
    "        pred_choice = out.max(1)[1]\n",
    "        correct = pred_choice.eq(cls.long().data).cpu().sum()\n",
    "        acc_cntr.append(correct.item()/sent.shape[0])\n",
    "\n",
    "        progress_bar(indx/len(test_loader), status=f\"test acc: {round(np.mean(acc_cntr), 4)}\")\n",
    "    progress_bar(1, status=f\"test acc: {round(np.mean(acc_cntr), 4)}\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
